# -*- coding: utf-8 -*-
"""task_1_a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LbvoUorhLiSP3ffnfCj1fXwLiNMha5SB
"""

!pip install ekphrasis

!pip install transformers==4.2.1

pip install tf-models-official

import tensorflow as tf
import os
import numpy as np
import pandas as pd
import string
from nltk.corpus import stopwords
import re
import os
from collections import Counter
from official import nlp
import official.nlp.optimization

import ekphrasis
from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])

tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

text_processor = TextPreProcessor(
    # terms that will be normalized
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
    # terms that will be annotated
    annotate={"hashtag", "allcaps", "elongated", "repeated",
        'emphasis', 'censored'},
    fix_html=True,  # fix HTML tokens
    segmenter="twitter",
    corrector="twitter",
    unpack_hashtags=True,  # perform word segmentation on hashtags
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=True,  # spell correction for elongated words
    tokenizer=SocialTokenizer(lowercase=True).tokenize,

    dicts=[emoticons]
)

#Mount the google drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/data'
# !ls

df_train = pd.read_csv('./train/train.csv', encoding='utf-8')
df_train['humor_rating'] = df_train['humor_rating'].fillna(0)
df_train['humor_controversy'] = df_train['humor_controversy'].fillna(2)
text_train = df_train["text"]
df_train.head(10)

df_val = pd.read_csv('./dev/dev.csv', encoding='utf-8')
df_val['humor_rating'] = df_val['humor_rating'].fillna(0)
df_val['humor_controversy'] = df_val['humor_controversy'].fillna(2)
text_val = df_val["text"]
df_val.head(10)

df_test = pd.read_csv('./test/public_test.csv', encoding='utf-8')
text_test = df_test["text"]
df_test.head(10)

print(len(text_train))
print(len(text_val))
print(len(text_test))

def print_text(texts,i,j):
    for u in range(i,j):
        print(texts[u])
        print()

# Functions for chat word conversion
f = open("./slang.txt", "r")
chat_words_str = f.read()
chat_words_map_dict = {}
chat_words_list = []

for line in chat_words_str.split("\n"):
    if line != "":
        cw = line.split("-")[0]
        cw_expanded = line.split("-")[1]
        chat_words_list.append(cw)
        chat_words_map_dict[cw] = cw_expanded
chat_words_list = set(chat_words_list)
def chat_words_conversion(text):
    new_text = []
    for w in text.split():
        if w.upper() in chat_words_list:
            new_text.append(chat_words_map_dict[w.upper()])
        else:
            new_text.append(w)
    return " ".join(new_text)

# Chat word conversion
# Training set
text_train = text_train.apply(lambda text: chat_words_conversion(text))
print_text(text_train,0,10)

print("----------------------------------------------------------------")

# Validation set
text_val = text_val.apply(lambda text: chat_words_conversion(text))
print_text(text_val,0,10)
print("----------------------------------------------------------------")
# Test set
text_test = text_test.apply(lambda text: chat_words_conversion(text))
# print_text(text_test,0,10)

def ekphrasis_pipe(sentence):
    cleaned_sentence = " ".join(text_processor.pre_process_doc(sentence))
    return cleaned_sentence

# Training set
text_train = text_train.apply(lambda text: ekphrasis_pipe(text))
print("Training set completed.......")
#Validation set
text_val = text_val.apply(lambda text: ekphrasis_pipe(text))
print("Validation set completed.......")
#Test set
text_test = text_test.apply(lambda text: ekphrasis_pipe(text))
print("Test set completed.......")

# Finding length of longest array
maxLen = len(max(text_train,key = lambda text: len(text.split(" "))).split(" "))

u = lambda text: len(text.split(" "))
sentence_lengths = []
for x in text_train:
    sentence_lengths.append(u(x))
print(len(sentence_lengths))

is_humor = df_train["is_humor"]
humor_rating = df_train["humor_rating"]
humor_controversy = df_train["humor_controversy"].astype(int)
offense_rating = df_train["offense_rating"]
print(Counter(is_humor))
print(Counter(humor_controversy))

is_humor_val = df_val["is_humor"]
humor_rating_val = df_val["humor_rating"]
humor_controversy_val = df_val["humor_controversy"].astype(int)
offense_rating_val = df_val["offense_rating"]
print(Counter(is_humor_val))
print(Counter(humor_controversy_val))

from transformers import RobertaTokenizerFast, TFRobertaModel, TFBertModel, BertTokenizerFast, ElectraTokenizerFast, TFElectraModel, AlbertTokenizerFast, TFAlbertModel, XLNetTokenizerFast, TFXLNetModel, MPNetTokenizerFast, TFMPNetModel
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import ModelCheckpoint

from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

# Define tokenizer as per requirement
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
text_train = list(text_train)
text_val = list(text_val)
text_test = list(text_test)
train_encodings = tokenizer(text_train, max_length=150, truncation=True, padding="max_length", return_tensors='tf')
val_encodings = tokenizer(text_val, max_length=150, truncation=True, padding="max_length", return_tensors='tf')
test_encodings = tokenizer(text_test, max_length=150, truncation=True, padding="max_length", return_tensors='tf')
print(np.shape(train_encodings["input_ids"]))
print(np.shape(val_encodings["input_ids"]))
print(np.shape(test_encodings["input_ids"]))
print(train_encodings["input_ids"][0])
print("***************************************************************************")
print(val_encodings["input_ids"][0])

def task_1(input_shape):
    model = TFBertModel.from_pretrained('bert-base-uncased')
    layer = model.layers[0]
    inputs = keras.Input(shape=input_shape, dtype='int32')
    input_masks = keras.Input(shape=input_shape, dtype='int32')

    outputs = layer([inputs, input_masks])
    output = outputs[0]
    pooled_output = output[:, 0, :]
    is_humor = layers.Dropout(0.3)(pooled_output)
    is_humor = layers.Dense(1, activation="sigmoid")(is_humor)

    model = keras.Model(inputs=[inputs,input_masks], outputs=is_humor, name='Task_1_a')
    
    return model



strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
    model = task_1((150,))
    optimizer = keras.optimizers.Adam(learning_rate=2e-5)
    loss_fun = [
          tf.keras.losses.BinaryCrossentropy(from_logits=False)
    ]
    metric = [
        tf.keras.metrics.BinaryAccuracy(),
        tf.keras.metrics.Precision(),
        tf.keras.metrics.Recall()
    ]
    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)

model.summary()

checkpoint = ModelCheckpoint(filepath='/content/task-1-a-model-name.{epoch:03d}.h5',
                                 verbose = 0,
                                 save_weights_only=True,
                                 epoch=4)
count_humor = Counter(is_humor)
zero = count_humor[0]
one = count_humor[1]
total = zero + one
print("Not Humorous: ",zero)
print("Humorous: ",one)
print("Total: ",total)

class_weight_ = {}
maxi = max(zero, one)
weight_for_0 = (maxi / (maxi + zero))
weight_for_1 = (maxi / (maxi + one))

class_weight_motivation = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

import time
start_time = time.time()
history_task_1_a = model.fit(
    x = [train_encodings["input_ids"], train_encodings["attention_mask"]],
    y = is_humor,
    validation_data = ([val_encodings["input_ids"],val_encodings["attention_mask"]], is_humor_val),
    callbacks = [checkpoint],
    batch_size=16,
    class_weight=class_weight_,
    shuffle=True,
    epochs=4)
print("--- %s seconds ---" % (time.time() - start_time))

import time
start_time = time.time()
val_answer = model.predict([val_encodings["input_ids"],val_encodings["attention_mask"]])
val_answer = np.round(val_answer)
val_answer = np.squeeze(val_answer, axis=-1)
print("--- %s seconds ---" % (time.time() - start_time))

from sklearn.metrics import classification_report
print(classification_report(is_humor_val, val_answer, digits=4))

test_answer = model.predict([test_encodings["input_ids"],test_encodings["attention_mask"]])
test_answer = np.round(test_answer)
test_answer = np.squeeze(test_answer, axis=-1)
np.shape(test_answer)

test_id = df_test["id"]

test_dict = {
    "id" : test_id,
    "is_humor" : test_answer
}

df_test = pd.DataFrame(test_dict)
df_test.head()
df_test.to_csv('answer-1-a.csv', index=False)

####### Task 1-B
def hahackathon_task_1(input_shape):
    # Import model as required
    model = TFAlbertModel.from_pretrained('albert-large-v2')
    layer = model.layers[0]
    # Model
    inputs = keras.Input(shape=input_shape, dtype='int32')
    input_masks = keras.Input(shape=input_shape, dtype='int32')

    outputs = layer([inputs, input_masks])
    output = outputs[0]
    # pooled_output = outputs[1]
    pooled_output = output[:, 0, :]  # Use for bert, roberta, albert, mpnet, electra
    # pooled_output = output[:, -1] #Use for XLNet

    # Humour regression
    humor_reg = layers.Dropout(0.3)(pooled_output)
    humor_reg = layers.Dense(1)(humor_reg)

    model = keras.Model(inputs=[inputs, input_masks], outputs=humor_reg, name='task_1_b')

    return model


strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
    model = hahackathon_task_1((150,))
    optimizer = keras.optimizers.Adam(learning_rate=3e-5)
    loss_fun = [
        tf.keras.losses.LogCosh()
    ]
    metric = [
        tf.keras.metrics.RootMeanSquaredError()
    ]
    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)

model.summary()

checkpoint = ModelCheckpoint(filepath='/content/task-1-b-albert.{epoch:03d}.h5',
                             verbose=0,
                             save_weights_only=True,
                             epoch=4)

# Albert base pooled
history_task_1_b = model.fit(
    x=[train_encodings["input_ids"], train_encodings["attention_mask"]],
    y=humor_rating,
    validation_data=([val_encodings["input_ids"], val_encodings["attention_mask"]], humor_rating_val),
    callbacks=[checkpoint],
    batch_size=16,
    shuffle=True,
    epochs=4)

from sklearn.metrics import mean_squared_error
from math import sqrt
import time

start_time = time.time()
val_answer = model.predict([val_encodings["input_ids"], val_encodings["attention_mask"]])
print("--- %s seconds ---" % (time.time() - start_time))

print("mean_squared_error: ")
sqrt(mean_squared_error(humor_rating_val, val_answer))

import time

start_time = time.time()
test_answer = model.predict([test_encodings["input_ids"], test_encodings["attention_mask"]])

test_1_b = []
for i in range(0, len(test_answer)):
    test_1_b.append(test_answer[i][0])
test_id = df_test["id"]

test_dict = {
    "id": test_id,
    "humor_rating": test_1_b
}
df_test = pd.DataFrame(test_dict)
df_test.head()

df_test.to_csv('answer-1-b.csv', index=False)



#### Task 1-C
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

text_train = list(text_train)
text_val = list(text_val)
text_test = list(text_test)

train_encodings = tokenizer(text_train, max_length=150, truncation=True, padding="max_length", return_tensors='tf')
val_encodings = tokenizer(text_val, max_length=150, truncation=True, padding="max_length", return_tensors='tf')
test_encodings = tokenizer(text_test, max_length=150, truncation=True, padding="max_length", return_tensors='tf')

print(np.shape(train_encodings["input_ids"]))
print(np.shape(val_encodings["input_ids"]))
print(np.shape(test_encodings["input_ids"]))

print(train_encodings["input_ids"][0])
print("***************************************************************************")
print(val_encodings["input_ids"][0])


def hahackathon_task_1(input_shape):
    model = TFBertModel.from_pretrained('bert-base-uncased')
    layer = model.layers[0]
    # Model
    inputs = keras.Input(shape=input_shape, dtype='int32')
    input_masks = keras.Input(shape=input_shape, dtype='int32')

    outputs = layer([inputs, input_masks])
    output = outputs[0]
    # pooled_output = outputs[1]
    pooled_output = output[:, 0, :]  # Use for bert, roberta, albert, mpnet, electra
    # pooled_output = output[:, -1] # Use for XLNet

    # Humour controversy
    humor_binary = layers.Dropout(0.3)(pooled_output)
    humor_binary = layers.Dense(1, activation="sigmoid")(humor_binary)

    model = keras.Model(inputs=[inputs, input_masks], outputs=humor_binary, name='Haha')

    return model


strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
    np.random.seed(45)
    model = hahackathon_task_1((150,))
    optimizer = keras.optimizers.Adam(learning_rate=2e-5)
    loss_fun = [
        tf.keras.losses.BinaryCrossentropy()
    ]
    metric = [
        tf.keras.metrics.BinaryAccuracy(),
        tf.keras.metrics.Precision(),
        tf.keras.metrics.Recall()
    ]
    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)

model.summary()

checkpoint = ModelCheckpoint(filepath='/content/task-1-c-model.{epoch:03d}.h5',
                             verbose=0,
                             save_weights_only=True,
                             epoch=4)

import time

start_time = time.time()
history_task_1_c = model.fit(
    x=[train_encodings["input_ids"], train_encodings["attention_mask"]],
    y=humor_controversy,
    validation_data=([val_encodings["input_ids"], val_encodings["attention_mask"]], humor_controversy_val),
    callbacks=[checkpoint],
    batch_size=16,
    shuffle=True,
    epochs=4)
print("--- %s seconds ---" % (time.time() - start_time))

val_answer = model.predict([val_encodings["input_ids"], val_encodings["attention_mask"]])

val_humor_controversy = []

for i in range(0, len(val_answer)):
    if val_answer[i] > 0.5:
        val_humor_controversy.append(1)
    else:
        val_humor_controversy.append(0)

con_mat_humor_controversy = tf.math.confusion_matrix(humor_controversy_val, val_humor_controversy, num_classes=None,
                                                     weights=None, dtype=tf.dtypes.int32)
print(con_mat_humor_controversy)

print(classification_report(humor_controversy_val, val_humor_controversy, digits=3))

import time

start_time = time.time()
test_answer = model.predict([test_encodings["input_ids"], test_encodings["attention_mask"]])
print("--- %s seconds ---" % (time.time() - start_time))

test_1_c = []

for i in range(0, len(test_answer)):
    if test_answer[i] > 0.5:
        test_1_c.append(1)
    else:
        test_1_c.append(0)

len(test_1_c)

test_id = df_test["id"]

test_dict = {
    "id": test_id,
    "humor_controversy": test_1_c
}

df_test = pd.DataFrame(test_dict)
df_test.head()

df_test.to_csv('albert-1-c.csv', index=False)

# Task 2

def hahackathon_task_2(input_shape):
    # Import model as required
    model = TFBertModel.from_pretrained('bert-base-uncased')
    layer = model.layers[0]
    # Model
    inputs = keras.Input(shape=input_shape, dtype='int32')
    input_masks = keras.Input(shape=input_shape, dtype='int32')

    outputs = layer([inputs, input_masks])
    output = outputs[0]
    # pooled_output = outputs[1]
    pooled_output = output[:, 0, :]  # Use for bert, roberta, albert, mpnet, electra
    # pooled_output = output[:, -1] #Use for XLNet

    # Humour regression
    humor_reg = layers.Dropout(0.3)(pooled_output)
    humor_reg = layers.Dense(1)(humor_reg)

    model = keras.Model(inputs=[inputs, input_masks], outputs=humor_reg, name='task_2')

    return model


strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
    model = hahackathon_task_2((150,))
    optimizer = keras.optimizers.Adam(learning_rate=3e-5)
    loss_fun = [
        tf.keras.losses.LogCosh()
    ]
    metric = [
        tf.keras.metrics.RootMeanSquaredError()
    ]
    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)

model.summary()

checkpoint = ModelCheckpoint(filepath='/content/task-2.{epoch:03d}.h5',
                             verbose=0,
                             save_weights_only=True,
                             epoch=4)

# albert base Pooled output

history_task_1_b = model.fit(
    x=[train_encodings["input_ids"], train_encodings["attention_mask"]],
    y=offense_rating,
    validation_data=([val_encodings["input_ids"], val_encodings["attention_mask"]], offense_rating_val),
    callbacks=[checkpoint],
    batch_size=16,
    shuffle=True,
    epochs=1)

val_answer = model.predict([val_encodings["input_ids"], val_encodings["attention_mask"]])

val_answer = np.where(val_answer < 0, 0, val_answer)

from sklearn.metrics import mean_squared_error
from math import sqrt

sqrt(mean_squared_error(offense_rating_val, val_answer))

import time

start_time = time.time()

test_answer = model.predict([test_encodings["input_ids"], test_encodings["attention_mask"]])
print("--- %s seconds ---" % (time.time() - start_time))

test_2 = []
for i in range(0, len(test_answer)):
    test_2.append(test_answer[i][0])

len(test_2)

test_id = df_test["id"]

test_dict = {
    "id": test_id,
    "offense_rating": test_2
}

df_test = pd.DataFrame(test_dict)
df_test.head()

df_test.to_csv('albert-2.csv', index=False)


